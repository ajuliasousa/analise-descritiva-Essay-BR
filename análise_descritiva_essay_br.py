# -*- coding: utf-8 -*-
"""análise-descritiva-Essay-BR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vj3MvAdhEW8xs8GZYzRqY0j9aby6G2M8

#**Análise descritiva do conjunto público de redações Essay-br**

##Importação de bibliotecas

Descrição das bibliotecas para cálculos, métricas e processamento de texto do dataset.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

# bibliotecas para processamento de texto
import nltk
import spacy

"""## Carregamento de dados

Os datasets foram colocados por upload no ambiente de desenvolvimento, logo em sequência é feita a leitura como forma de conferência das variáveis presentes nas bases `essays-br.csv` e `prompts.csv`.
"""

essays_df = pd.read_csv('essay-br.csv')
print(essays_df.head())

prompts_df = pd.read_csv('prompts.csv')
print(prompts_df.head())

"""## Análise Exploratória dos Dados (EDA)

Informações sobre o dataset `essay-br.csv`
"""

print(essays_df.info())
print(essays_df.shape)
print(essays_df.isnull().sum())

print(prompts_df.info())
print(prompts_df.shape)
print(prompts_df.isnull().sum())

#tratamento de duplicatas
essays_df.drop_duplicates(inplace=True)
prompts_df.drop_duplicates(inplace=True)

"""### Distribuições das pontuações

O gráfico mostra uma distribuição assimétrica à esquerda, onde a maioria das pontuações se concentra em torno de 600 (a medida da moda). Há variações consideráveis em outros intervalos, especialmente entre 400 e 800 pontos, onde a frequência das pontuações varia mais.
"""

# gráfico das pontuações - visualizar formato da distribuição
plt.figure(figsize=(10, 6))
sns.histplot(essays_df['score'], bins=20, kde=True)
plt.title('Distribuição das Pontuações')
plt.xlabel('Score')
plt.ylabel('Frequência')
plt.show()

#estatísticas descritivas da distribuição
print(essays_df['score'].describe())

"""### Distribuições dos tópicos (temas)

Alguns tópicos têm uma contagem muito alta de ensaios, indicando que são temas populares entre os participantes. Por exemplo, os tópicos nos intervalos em torno de 45-55 e 80-90, com picos acima de 120 redações.
"""

# Contagem de ensaios por tema de eixo
prompt_counts = essays_df['prompt'].value_counts()

plt.figure(figsize=(14, 7))
sns.barplot(x=prompt_counts.index, y=prompt_counts.values)
plt.xticks(rotation=90)
plt.title('Contagem de Ensaios por Tópico')
plt.xlabel('Tópico')
plt.ylabel('Contagem')
plt.show()

"""O gráfico apresenta a variação das pontuações para diferentes tópicos, com caixas representando o intervalo interquartil (IQR), linhas indicando a faixa de dados dentro de 1,5 vezes o IQR, e pontos indicando outliers. As linhas dentro das caixas representam a mediana das pontuações para cada tópico."""

# Distribuição das pontuações por tema
plt.figure(figsize=(14, 7))
sns.boxplot(x='prompt', y='score', data=essays_df)
plt.xticks(rotation=90)
plt.title('Distribuição das Pontuações por Tópico')
plt.xlabel('Tópico')
plt.ylabel('Pontuação')
plt.show()

"""Uma possibilidade é azer uma análise qualitativa detalhada dos ensaios em tópicos populares versus impopulares para entender melhor as diferenças na qualidade e nos desafios enfrentados pelos alunos.

### Análise de texto

#### Tokenização e remoção de stop words

**Tokenização:** Processo de dividir o texto em unidades menores chamadas "tokens", que geralmente são palavras ou frases curtas. <br>
**Remoção de Stop Words:** Stop words são palavras comuns (como "a", "de", "é") que geralmente não adicionam significado significativo ao texto.
"""

!pip install spacy
!python -m spacy download pt_core_news_sm

import nltk
nltk.download('stopwords')

nlp = spacy.load("pt_core_news_sm")
stop_words = nltk.corpus.stopwords.words('portuguese')

all_texts = essays_df['essay'].tolist()
all_tokens = []

for text in tqdm(all_texts):
  doc = nlp(text)
  tokens = [token.text.lower() for token in doc if token.is_alpha and token.text.lower() not in stop_words]
  all_tokens.extend(tokens)

"""#### Frequência de palavras

Refere-se ao número de vezes que cada palavra aparece em um texto ou no conjunto de textos, permitindo insights sobre os temas e tópicos predominantes. A nuvem de palavras é a representação visual a partir da frequência obtida.
"""

from collections import Counter
word_freq = Counter(all_tokens)
print(word_freq.most_common(20))

"""#### Nuvem de palavras"""

from wordcloud import WordCloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Nuvem de Palavras')
plt.show()

"""#### Bigramas e trigramas

São sequências de duas ou três palavras adjacentes em um texto, para captura de contexto  e identificação de expressóes.
"""

from nltk import bigrams, trigrams
bigrams_freq = Counter(bigrams(all_tokens))
trigrams_freq = Counter(trigrams(all_tokens))

print("Bigramas mais frequentes:", bigrams_freq.most_common(10))
print("Trigramas mais frequentes:", trigrams_freq.most_common(10))

"""#### Modelagem de tópicos (Topic Modeling) - LDA (Latent Dirichlet Allocation)

Inicia-se importando as bibliotecas necessárias para a vetorização de texto e a decomposição de tópicos e cria-se um vetorizador que transforma os textos em uma matriz de contagem de palavras. Tem-se então um modelo LDA para encontrar 5 tópicos nos textos, com as dez palavras mais significativas.
"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')
dtm = vectorizer.fit_transform(all_texts)

lda = LatentDirichletAllocation(n_components=5, random_state=42)
lda.fit(dtm)

for index, topic in enumerate(lda.components_):
    print(f'Tópico #{index}:')
    print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]])
    print('\n')

"""Existe a possibilidade dos textos no dataset serem muito similares compalavras comuns e repetitivas, isso pode afetando a qualidade dos tópicos gerados.

#### Análise de entidades nomeadas (Named Entity Recognition - NER)

Tratativa para identificar e classificar entidades em um texto em categorias predefinidas, como pessoas, organizações, locais, datas e outros tipos de entidades.
"""

all_entities = []
for text in tqdm(all_texts):
  doc = nlp(text)
  entities = [(ent.text, ent.label_) for ent in doc.ents]
  all_entities.extend(entities)

entity_freq = Counter(all_entities)
print(entity_freq.most_common(20))

"""A saída mostra pares de (entidade, categoria) e a contagem de ocorrências dessa entidade em seu texto.

#### Análise de sentimentos

O código analisa cada texto no conjunto all_texts para determinar o sentimento (polaridade) e a subjetividade, armazenando os resultados em um DataFrame. Quanto à visualização, plota histogramas da distribuição de ambos.
"""

from textblob import TextBlob

sentiments = []
subjectivities = []
for text in tqdm(all_texts):
  blob = TextBlob(text)
  sentiments.append(blob.sentiment.polarity)
  subjectivities.append(blob.sentiment.subjectivity)

essays_df['sentiment'] = sentiments
essays_df['subjectivity'] = subjectivities

# plotagem da distribuição
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.histplot(essays_df['sentiment'], bins=20, kde=True)
plt.title('Distribuição de Sentimentos')
plt.xlabel('Sentimento')
plt.ylabel('Frequência')

plt.subplot(1, 2, 2)
sns.histplot(essays_df['subjectivity'], bins=20, kde=True)
plt.title('Distribuição de Subjetividade')
plt.xlabel('Subjetividade')
plt.ylabel('Frequência')

plt.tight_layout()
plt.show()

"""O eixo X no primeiro gráfico representa o valor de polaridade do sentimento, que varia de -1 (muito negativo) a +1 (muito positivo). O eixo X no segundo gráfico representa o valor de subjetividade, que varia de 0 (objetivo) a 1 (subjetivo).  O eixo Y mostra a frequência ou o número de textos que possuem tais características.

#### Análise de complexidade sintática

Objetivo de analisar como as palavras e frases estão organizadas e como isso afeta a dificuldade de compreensão e processamento do texto.
"""

def analyze_syntax(text):
  doc = nlp(text)
  num_sentences = len(list(doc.sents))
  num_words = len([token for token in doc if token.is_alpha])
  avg_sentence_length = num_words / num_sentences if num_sentences > 0 else 0
  return num_sentences, avg_sentence_length

sentence_counts = []
avg_sentence_lengths = []
for text in tqdm(all_texts):
  num_sentences, avg_sentence_length = analyze_syntax(text)
  sentence_counts.append(num_sentences)
  avg_sentence_lengths.append(avg_sentence_length)

essays_df['num_sentences'] = sentence_counts
essays_df['avg_sentence_length'] = avg_sentence_lengths

# Visualização da complexidade sintática
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.histplot(essays_df['num_sentences'], bins=20, kde=True)
plt.title('Distribuição do Número de Sentenças')
plt.xlabel('Número de Sentenças')
plt.ylabel('Frequência')

plt.subplot(1, 2, 2)
sns.histplot(essays_df['avg_sentence_length'], bins=20, kde=True)
plt.title('Distribuição do Comprimento Médio das Sentenças')
plt.xlabel('Comprimento Médio das Sentenças')
plt.ylabel('Frequência')

plt.tight_layout()
plt.show()

"""O pico em torno de 20-30 caracteres indica que muitas sentenças têm um comprimento médio relativamente curto.

#### Análise de Correlação
"""

# matriz de correlação entre características e score
correlation_matrix = essays_df[['score', 'sentiment', 'subjectivity', 'num_sentences', 'avg_sentence_length']].corr()
sns.heatmap(correlation_matrix, annot=True)
plt.title('Matriz de Correlação')
plt.show()

from scipy.stats import pearsonr, spearmanr

corr_score_sentiment, p_value_score_sentiment = pearsonr(essays_df['score'], essays_df['sentiment'])
corr_score_subjectivity, p_value_score_subjectivity = pearsonr(essays_df['score'], essays_df['subjectivity'])
corr_score_num_sentences, p_value_score_num_sentences = pearsonr(essays_df['score'], essays_df['num_sentences'])
corr_score_avg_sentence_length, p_value_score_avg_sentence_length = pearsonr(essays_df['score'], essays_df['avg_sentence_length'])

print("Correlação entre pontuação e sentimento:", corr_score_sentiment, "Valor-p:", p_value_score_sentiment)
print("Correlação entre pontuação e subjetividade:", corr_score_subjectivity, "Valor-p:", p_value_score_subjectivity)
print("Correlação entre pontuação e número de sentenças:", corr_score_num_sentences, "Valor-p:", p_value_score_num_sentences)
print("Correlação entre pontuação e comprimento médio das sentenças:", corr_score_avg_sentence_length, "Valor-p:", p_value_score_avg_sentence_length)

"""### Suposições e questões

Notas sobre dúvidas para correção automática de redações

*   Considerando os critérios de correção do ENEM passam por dois avaliadores em busca de senso comum para média (com possibilidade de um terceiro), como pontuar em caso que o aluno peça revisão da nota obtida?

*   Possibilidade de identificação de cópia do texto (podendo ser dos colocados como referência ou de outra parte da prova)?

*    Identificação de atendimento ao tipo textual argumentativo?

*    Como lidar com ambiguidades e subjetividade na composição do texto?

*    Como tratar erros gramaticais e de coesão para definição da pontuação (a nível de competência e soma total dos critérios)?

*    Possibilidade de feedback para o aluno a partir de comparação com os critérios de avaliação

###Plano de solução

**Etapas:**


1.   **Definição de Requisitos:** definição de critérios de avaliação trazidos pela cartilha de correção do ENEM e escala de notas. Além de identificação do tipo argumentativo como validação.
2.   **Coleta de dados:** banco de redações corrigidas por avaliadores humanos, com notas atribuídas para treinamento dos modelos. Dar atenção para garantia de diversidade de temas.
3.   **Pré-processamento:** para o tratamento dos dados com identificação de valores faltantes, remoção de ruídos, tokenização, remoção de stopwords, extração de características como contagem de palavras e estrutura de parágrafos.
4.   **Stack tecnológica:** uso da linguagem de programção Python,por bibliotecas de Processamento de Linguagem Natural e aprendizado de máquina - como NLTK, spaCy, scikit-learn, TensorFlow, Pytorch. Como ferramentas de análise semântica pode-se utilizar Word2Vec ou Glove para representação em formato vetorial e o BERT para compreensão de contexto. Para o desenvolvimento, tem-se o Jupyter Notebook e o Google Colab, esse último com suporte a GPUs;
5.   **Técnicas e Algoritmos:** Para a extração de características sintáticas e semânticas pode-se utilizar n-grams, TF-IDF(Term Frequency-Inverse Document Frequency). Então, os modelos de avaliação como regressão linear ou logística para modelagem inicial, random forest e gradient boosting para classificação e regressão, redes neurais convolucionais (CNN) e recorrentes (RNN) para entendimento de dependências contextuais e sequenciais. Para análise de mais complexas, o uso de transformers como o modelo BERT.
6.   **Validação e Teste:** validação cruzada (K-fold) para observação de desempenho do modelo e métricas de avaliação como MSE (Mean Squared Error), RMSE (Root Mean Squared Error), precisão, recall, F1-score, etc.

### Referências Bibliográficas

BRASIL. Instituto Nacional de Estudos e Pesquisas Educacionais Anísio Teixeira (Inep). A Redação do Enem 2023: cartilha do participante. Brasília, 2023.

MARINHO, Jeziel C.; ANCHIÊTA, Rafael T.; MOURA, Raimundo S.. Essay-BR: a Brazilian Corpus of Essays. In: DATASET SHOWCASE WORKSHOP (DSW), 3. , 2021, Rio de Janeiro. Anais [...]. Porto Alegre: Sociedade Brasileira de Computação, 2021 . p. 53-64. DOI: https://doi.org/10.5753/dsw.2021.17414.

RASSI, Amanda Pontes; DE ABREU LOPES, Priscilla. Capítulo 19: Correção automática de redação. 2023.
"""